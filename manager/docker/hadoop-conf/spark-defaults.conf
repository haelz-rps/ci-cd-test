spark.master=local[2]

# hive config
# avoid using hive shim
spark.sql.catalogImplementation=hive
spark.hadoop.hive.metastore.uris=thrift://metastore:9083
spark.hive.metastore.uris=thrift://metastore:9083
spark.sql.statistics.histogram.enabled=true

# default aws fs config
spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.committer.name=magic
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem

# hadoop cloud specific config
spark.hadoop.mapreduce.manifest.committer.cleanup.parallel.delete.base.first=false
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored=true
spark.hadoop.parquet.enable.summary-metadata=false
spark.sql.sources.commitProtocolClass=org.apache.spark.internal.io.cloud.PathOutputCommitProtocol
spark.sql.parquet.output.committer.class=org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter

# ivy folder error fix
spark.jars.ivy=/tmp/.ivy
